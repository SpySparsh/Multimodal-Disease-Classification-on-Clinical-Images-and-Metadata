{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be17fc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All model classes imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# 04_train_prototype.ipynb\n",
    "# Training + evaluation prototype with train/val split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, multilabel_confusion_matrix\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the notebook directory to path to import from previous cells\n",
    "sys.path.append('.')\n",
    "\n",
    "# Import our custom classes from the previous notebook\n",
    "# We'll recreate them here for this training script\n",
    "\n",
    "# ---- Dataset class (from 03_dataset_prototype.ipynb) ----\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "class ChestXrayDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"PyTorch Dataset for Chest X-ray images with multi-hot encoded labels.\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_path: str, images_dir: str, class_to_idx_path: str = None):\n",
    "        self.metadata_path = metadata_path\n",
    "        self.images_dir = images_dir\n",
    "        self.class_to_idx_path = class_to_idx_path or \"../data/class_to_idx.json\"\n",
    "        \n",
    "        # Load metadata\n",
    "        self.metadata = pd.read_csv(metadata_path)\n",
    "        \n",
    "        # Build class_to_idx mapping if not exists\n",
    "        if os.path.exists(self.class_to_idx_path):\n",
    "            with open(self.class_to_idx_path, 'r') as f:\n",
    "                self.class_to_idx = json.load(f)\n",
    "        else:\n",
    "            self.class_to_idx = self._build_class_mapping()\n",
    "            self._save_class_mapping()\n",
    "        \n",
    "        # Build view mapping\n",
    "        self.view_to_idx = self._build_view_mapping()\n",
    "        \n",
    "        # Image preprocessing transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "        ])\n",
    "        \n",
    "        # Filter out samples where image doesn't exist\n",
    "        self.valid_indices = self._filter_valid_samples()\n",
    "        \n",
    "    def _build_class_mapping(self) -> Dict[str, int]:\n",
    "        \"\"\"Build class_to_idx mapping from finding column.\"\"\"\n",
    "        all_classes = set()\n",
    "        \n",
    "        for finding in self.metadata['finding'].dropna():\n",
    "            # Split on '/' for multiple diseases\n",
    "            classes = [cls.strip() for cls in finding.split('/')]\n",
    "            all_classes.update(classes)\n",
    "        \n",
    "        # Sort for consistent ordering\n",
    "        sorted_classes = sorted(list(all_classes))\n",
    "        return {cls: idx for idx, cls in enumerate(sorted_classes)}\n",
    "    \n",
    "    def _build_view_mapping(self) -> Dict[str, int]:\n",
    "        \"\"\"Build view_to_idx mapping from view column.\"\"\"\n",
    "        unique_views = sorted(self.metadata['view'].dropna().unique())\n",
    "        return {view: idx for idx, view in enumerate(unique_views)}\n",
    "    \n",
    "    def _save_class_mapping(self):\n",
    "        \"\"\"Save class_to_idx mapping to JSON file.\"\"\"\n",
    "        os.makedirs(os.path.dirname(self.class_to_idx_path), exist_ok=True)\n",
    "        with open(self.class_to_idx_path, 'w') as f:\n",
    "            json.dump(self.class_to_idx, f, indent=2)\n",
    "    \n",
    "    def _filter_valid_samples(self) -> List[int]:\n",
    "        \"\"\"Filter out samples where image file doesn't exist.\"\"\"\n",
    "        valid_indices = []\n",
    "        for idx, row in self.metadata.iterrows():\n",
    "            image_path = os.path.join(self.images_dir, row['filename'])\n",
    "            if os.path.exists(image_path):\n",
    "                valid_indices.append(idx)\n",
    "        return valid_indices\n",
    "    \n",
    "    def _encode_finding(self, finding: str) -> torch.Tensor:\n",
    "        \"\"\"Convert finding string to multi-hot encoded vector.\"\"\"\n",
    "        if pd.isna(finding):\n",
    "            return torch.zeros(len(self.class_to_idx), dtype=torch.float32)\n",
    "        \n",
    "        # Split on '/' for multiple diseases\n",
    "        classes = [cls.strip() for cls in finding.split('/')]\n",
    "        \n",
    "        # Create multi-hot vector\n",
    "        label_vector = torch.zeros(len(self.class_to_idx), dtype=torch.float32)\n",
    "        for cls in classes:\n",
    "            if cls in self.class_to_idx:\n",
    "                label_vector[self.class_to_idx[cls]] = 1.0\n",
    "        \n",
    "        return label_vector\n",
    "    \n",
    "    def _encode_view(self, view: str) -> torch.Tensor:\n",
    "        \"\"\"Convert view string to one-hot encoded vector.\"\"\"\n",
    "        if pd.isna(view):\n",
    "            return torch.zeros(len(self.view_to_idx), dtype=torch.float32)\n",
    "        \n",
    "        view_vector = torch.zeros(len(self.view_to_idx), dtype=torch.float32)\n",
    "        if view in self.view_to_idx:\n",
    "            view_vector[self.view_to_idx[view]] = 1.0\n",
    "        \n",
    "        return view_vector\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get a sample from the dataset. This version handles missing/corrupt images gracefully by\n",
    "        falling back to a black placeholder image and logging a warning instead of raising an error.\n",
    "        \"\"\"\n",
    "        # Get the actual index in metadata\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        row = self.metadata.iloc[actual_idx]\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image_path = os.path.join(self.images_dir, row['filename'])\n",
    "        placeholder = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(image_path):\n",
    "                # If missing, use placeholder and warn\n",
    "                print(f\"‚ö†Ô∏è Missing image, using placeholder: {image_path}\")\n",
    "                image = placeholder\n",
    "            else:\n",
    "                # Try to open the image; handle corrupted files\n",
    "                try:\n",
    "                    image = Image.open(image_path).convert('RGB')\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Failed to open image {image_path}: {e}. Using placeholder.\")\n",
    "                    image = placeholder\n",
    "        except Exception as e:\n",
    "            # Any unexpected error, fall back to placeholder\n",
    "            print(f\"‚ö†Ô∏è Error accessing image {image_path}: {e}. Using placeholder.\")\n",
    "            image = placeholder\n",
    "\n",
    "        # Apply transforms\n",
    "        try:\n",
    "            image_tensor = self.transform(image)\n",
    "        except Exception as e:\n",
    "            # If transform fails for any reason, fall back to zeros tensor of expected shape\n",
    "            print(f\"‚ö†Ô∏è Transform failed for {image_path}: {e}. Using zero tensor.\")\n",
    "            image_tensor = torch.zeros(3, 224, 224, dtype=torch.float32)\n",
    "\n",
    "        # Encode metadata and labels\n",
    "        metadata_tensor = self._encode_view(row['view'])\n",
    "        label_tensor = self._encode_finding(row['finding'])\n",
    "        \n",
    "        return image_tensor, metadata_tensor, label_tensor\n",
    "    \n",
    "    def get_class_names(self) -> List[str]:\n",
    "        \"\"\"Get list of class names in order.\"\"\"\n",
    "        return [cls for cls, _ in sorted(self.class_to_idx.items(), key=lambda x: x[1])]\n",
    "\n",
    "# ---- Model classes (from 03_dataset_prototype.ipynb) ----\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Image encoder using EfficientNet-B0 pretrained model.\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained: bool = True):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        \n",
    "        # Load pretrained EfficientNet-B0\n",
    "        self.backbone = models.efficientnet_b0(pretrained=pretrained)\n",
    "        \n",
    "        # Freeze all parameters in the backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Replace the classifier with Identity to output raw features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        # Get the feature dimension (1280 for EfficientNet-B0)\n",
    "        self.feature_dim = 1280\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)\n",
    "\n",
    "class MetadataEncoder(nn.Module):\n",
    "    \"\"\"Metadata encoder using a simple feedforward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 7, hidden_dim: int = 64, output_dim: int = 128):\n",
    "        super(MetadataEncoder, self).__init__()\n",
    "        \n",
    "        # Define the feedforward network\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.encoder(x)\n",
    "\n",
    "class FusionClassifier(nn.Module):\n",
    "    \"\"\"Fusion classifier that combines image and metadata features.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_feat_dim: int = 1280, meta_feat_dim: int = 128, \n",
    "                 num_classes: int = 28, hidden_dim: int = 512, \n",
    "                 attention_dim: int = 256, dropout: float = 0.3):\n",
    "        super(FusionClassifier, self).__init__()\n",
    "        \n",
    "        self.img_feat_dim = img_feat_dim\n",
    "        self.meta_feat_dim = meta_feat_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        # Gating mechanism\n",
    "        self.gate = nn.Linear(meta_feat_dim, img_feat_dim)\n",
    "        \n",
    "        # Attention mechanism - project to common space\n",
    "        self.img_proj = nn.Linear(img_feat_dim, attention_dim)\n",
    "        self.meta_proj = nn.Linear(meta_feat_dim, attention_dim)\n",
    "        \n",
    "        # Fusion dimension after concatenation\n",
    "        fusion_dim = img_feat_dim + meta_feat_dim + attention_dim  # 1280 + 128 + 256 = 1664\n",
    "        \n",
    "        # Feedforward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, img_feat: torch.Tensor, meta_feat: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = img_feat.size(0)\n",
    "        \n",
    "        # 1. Gating mechanism\n",
    "        gate = torch.sigmoid(self.gate(meta_feat))  # (B, 1280)\n",
    "        gated_img = img_feat * gate  # (B, 1280)\n",
    "        \n",
    "        # 2. Attention mechanism\n",
    "        # Project both features to common space\n",
    "        img_proj = self.img_proj(gated_img)  # (B, 256)\n",
    "        meta_proj = self.meta_proj(meta_feat)  # (B, 256)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = F.softmax(torch.sum(img_proj * meta_proj, dim=1, keepdim=True), dim=0)  # (B, 1)\n",
    "        \n",
    "        # Apply attention to image features\n",
    "        attended_img = img_proj * attention_weights  # (B, 256)\n",
    "        \n",
    "        # 3. Concatenation fusion\n",
    "        fusion_vector = torch.cat([gated_img, meta_feat, attended_img], dim=1)  # (B, 1280 + 128 + 256)\n",
    "        \n",
    "        # 4. Feedforward classifier\n",
    "        logits = self.classifier(fusion_vector)  # (B, num_classes)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"‚úÖ All model classes imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442f49a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reproducibility ensured by setting global seed to 42\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# ‚úÖ Set Seed for Reproducibility\n",
    "# ==================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define a seed for all random operations\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Sets the seed for all libraries to ensure reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    # The two lines below are for GPU-specific reproducibility\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # These two lines ensure that CUDA operations are deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Apply the seed\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"‚úÖ Reproducibility ensured by setting global seed to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "907b258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata from ../data/metadata_sample_minimal.csv ‚Äî 950 rows\n",
      "Saved cleaned minimal metadata to ../data/metadata_sample_minimal_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Load minimal metadata and extract classes ---\n",
    "import pandas as pd\n",
    "\n",
    "meta_path = \"../data/metadata_sample_minimal.csv\"\n",
    "df = pd.read_csv(meta_path)\n",
    "print(f\"Loaded metadata from {meta_path} ‚Äî {len(df)} rows\")\n",
    "\n",
    "# Remove rows without finding and normalize strings\n",
    "df = df[df[\"finding\"].notna()].copy()\n",
    "df[\"finding\"] = df[\"finding\"].astype(str).str.strip()\n",
    "\n",
    "# Save a small copy to use downstream if needed\n",
    "small_meta_out = \"../data/metadata_sample_minimal_cleaned.csv\"\n",
    "df.to_csv(small_meta_out, index=False)\n",
    "print(f\"Saved cleaned minimal metadata to {small_meta_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a7828f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NIH metadata from: ../data/nih_chestxray_single.csv\n",
      "Renaming columns for compatibility...\n",
      "\n",
      "‚úÖ Data preparation complete. DataFrame is prepped and saved to ../data/nih_metadata_prepped.csv\n",
      "\n",
      "--- Class Distribution ---\n",
      "label\n",
      "No Finding      60361\n",
      "Infiltration    13869\n",
      "Atelectasis     11559\n",
      "Effusion        10042\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# ‚úÖ NIH 4-Class Problem: Data Preprocessing\n",
    "# ==================================================\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Define the path to YOUR new cleaned CSV file ---\n",
    "# Make sure this file is accessible by the notebook.\n",
    "raw_nih_metadata_path = \"../data/nih_chestxray_single.csv\" # <--- !!! UPDATE THIS PATH !!!\n",
    "\n",
    "# --- 2. Load your new dataset ---\n",
    "print(f\"Loading NIH metadata from: {raw_nih_metadata_path}\")\n",
    "df = pd.read_csv(raw_nih_metadata_path)\n",
    "\n",
    "# --- 3. Rename columns to match what the rest of the script expects ---\n",
    "# This is the key step to ensure minimal changes later on.\n",
    "print(\"Renaming columns for compatibility...\")\n",
    "df.rename(columns={\n",
    "    'Image Index': 'filename',\n",
    "    'View Position': 'view',\n",
    "    'single_label': 'label'\n",
    "}, inplace=True)\n",
    "\n",
    "# --- 4. Save the prepared file for the next steps to use ---\n",
    "# The rest of the notebook will load this prepped file.\n",
    "prepped_metadata_path = \"../data/nih_metadata_prepped.csv\"\n",
    "os.makedirs(os.path.dirname(prepped_metadata_path), exist_ok=True)\n",
    "df.to_csv(prepped_metadata_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Data preparation complete. DataFrame is prepped and saved to {prepped_metadata_path}\")\n",
    "print(\"\\n--- Class Distribution ---\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de61f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease priority for labeling: ['COVID-19', 'Viral', 'Pneumonia']\n",
      "\n",
      "Testing new relabeling function:\n",
      "'Pneumonia/Viral/COVID-19' -> 'COVID-19'\n",
      "'Pneumonia/Viral' -> 'Viral'\n",
      "'Pneumonia/Bacterial' -> 'Pneumonia'\n",
      "'Tuberculosis' -> 'Other'\n",
      "'No Finding' -> 'Other'\n",
      "\n",
      "‚úÖ New relabeling function created successfully!\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL CHANGE: Define the priority of diseases for labeling.\n",
    "# We must check for the most specific disease (COVID-19) first to ensure correct labeling.\n",
    "disease_priority_list = ['COVID-19', 'Viral', 'Pneumonia']\n",
    "print(f\"Disease priority for labeling: {disease_priority_list}\")\n",
    "\n",
    "\n",
    "# Create the new, corrected relabeling function\n",
    "def relabel_multi_class(finding_string, priority_list):\n",
    "    \"\"\"\n",
    "    Relabel findings for multi-class problem based on a priority list.\n",
    "    - Checks for diseases in the order specified in the priority_list.\n",
    "    - Returns the first match found, ensuring more specific diseases are labeled correctly.\n",
    "    - If no match is found, returns 'Other'.\n",
    "    \"\"\"\n",
    "    if pd.isna(finding_string):\n",
    "        return 'Other'\n",
    "\n",
    "    # Convert to string, split by '/', and strip whitespace from each part\n",
    "    finding_parts = [part.strip() for part in str(finding_string).split('/')]\n",
    "\n",
    "    # Check for diseases in the specified priority order\n",
    "    for disease in priority_list:\n",
    "        if disease in finding_parts:\n",
    "            return disease\n",
    "\n",
    "    # If no match from the priority list is found, return 'Other'\n",
    "    return 'Other'\n",
    "\n",
    "\n",
    "# Test the new function to demonstrate the corrected logic\n",
    "print(\"\\nTesting new relabeling function:\")\n",
    "test_cases = [\n",
    "    \"Pneumonia/Viral/COVID-19\", # Should now be 'COVID-19'\n",
    "    \"Pneumonia/Viral\",          # Should now be 'Viral'\n",
    "    \"Pneumonia/Bacterial\",      # Should remain 'Pneumonia'\n",
    "    \"Tuberculosis\",             # Should remain 'Other'\n",
    "    \"No Finding\",               # Should remain 'Other'\n",
    "]\n",
    "\n",
    "for test_case in test_cases:\n",
    "    # Use the new function and the new priority list\n",
    "    result = relabel_multi_class(test_case, disease_priority_list)\n",
    "    print(f\"'{test_case}' -> '{result}'\")\n",
    "\n",
    "print(f\"\\n‚úÖ New relabeling function created successfully!\")\n",
    "\n",
    "# IMPORTANT: We need to rename the function and list for the next cell to use\n",
    "relabel_4class = relabel_multi_class\n",
    "top_3_list = disease_priority_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c002ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 4-class labeled data to ../data/metadata_4_classes.csv...\n",
      "‚úÖ Successfully saved 95831 rows to ../data/metadata_4_classes.csv\n",
      "\n",
      "Verification - checking saved file:\n",
      "Loaded 95831 rows from saved file\n",
      "Columns: ['filename', 'Finding Labels', 'Follow-up #', 'Patient ID', 'Patient Age', 'Patient Gender', 'view', 'OriginalImage[Width', 'Height]', 'OriginalImagePixelSpacing[x', 'y]', 'label']\n",
      "Label distribution in saved file:\n",
      "label\n",
      "No Finding      60361\n",
      "Infiltration    13869\n",
      "Atelectasis     11559\n",
      "Effusion        10042\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üéâ 4-Class preprocessing completed successfully!\n",
      "üìÅ Output file: ../data/metadata_4_classes.csv\n",
      "üìä 4 classes: ['Atelectasis', 'Effusion', 'Infiltration', 'No Finding']\n",
      "üìà Class distribution:\n",
      "   No Finding: 60361 samples (63.0%)\n",
      "   Infiltration: 13869 samples (14.5%)\n",
      "   Atelectasis: 11559 samples (12.1%)\n",
      "   Effusion: 10042 samples (10.5%)\n"
     ]
    }
   ],
   "source": [
    "# Save the dataframe with 4-class labels\n",
    "output_path = \"../data/metadata_4_classes.csv\"\n",
    "print(f\"Saving 4-class labeled data to {output_path}...\")\n",
    "\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Successfully saved {len(df)} rows to {output_path}\")\n",
    "\n",
    "# Verify the saved file\n",
    "print(f\"\\nVerification - checking saved file:\")\n",
    "saved_df = pd.read_csv(output_path)\n",
    "print(f\"Loaded {len(saved_df)} rows from saved file\")\n",
    "print(f\"Columns: {list(saved_df.columns)}\")\n",
    "print(f\"Label distribution in saved file:\")\n",
    "print(saved_df['label'].value_counts())\n",
    "\n",
    "print(f\"\\nüéâ 4-Class preprocessing completed successfully!\")\n",
    "print(f\"üìÅ Output file: {output_path}\")\n",
    "print(f\"üìä 4 classes: {sorted(df['label'].unique())}\")\n",
    "print(f\"üìà Class distribution:\")\n",
    "for label, count in df['label'].value_counts().items():\n",
    "    print(f\"   {label}: {count} samples ({count/len(df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71736bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modified ChestXrayDatasetSingleLabel class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# ‚úÖ Modified Dataset Class for Single-Label Classification\n",
    "# ==================================================\n",
    "\n",
    "class ChestXrayDatasetSingleLabel(torch.utils.data.Dataset):\n",
    "    \"\"\"PyTorch Dataset for Chest X-ray images with single-label classification (4-class problem).\"\"\"\n",
    "    \n",
    "    # --- MODIFIED: The __init__ method now accepts a 'transform' argument ---\n",
    "    def __init__(self, metadata_path: str, images_dir: str, class_to_idx_path: str = None, transform=None):\n",
    "        self.metadata_path = metadata_path\n",
    "        self.images_dir = images_dir\n",
    "        self.class_to_idx_path = class_to_idx_path or \"../data/class_to_idx_4class.json\"\n",
    "        \n",
    "        # Load metadata\n",
    "        self.metadata = pd.read_csv(metadata_path)\n",
    "        print(f\"Loaded metadata with {len(self.metadata)} rows\")\n",
    "        \n",
    "        if 'label' not in self.metadata.columns:\n",
    "            raise ValueError(\"Metadata must contain 'label' column for single-label classification\")\n",
    "        \n",
    "        # Build class_to_idx mapping from the 'label' column\n",
    "        if os.path.exists(self.class_to_idx_path):\n",
    "            with open(self.class_to_idx_path, 'r') as f:\n",
    "                self.class_to_idx = json.load(f)\n",
    "            print(f\"Loaded existing class mapping: {self.class_to_idx}\")\n",
    "        else:\n",
    "            self.class_to_idx = self._build_class_mapping()\n",
    "            self._save_class_mapping()\n",
    "            print(f\"Built new class mapping: {self.class_to_idx}\")\n",
    "        \n",
    "        # Build view mapping\n",
    "        self.view_to_idx = self._build_view_mapping()\n",
    "        \n",
    "        # --- MODIFIED: Use the provided transform, or create a default one ---\n",
    "        if transform:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            # Default transform (for validation/testing if none is provided)\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                   std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "        # Filter out samples where image doesn't exist\n",
    "        self.valid_indices = self._filter_valid_samples()\n",
    "        print(f\"Found {len(self.valid_indices)} valid samples\")\n",
    "        \n",
    "    def _build_class_mapping(self) -> Dict[str, int]:\n",
    "        unique_labels = sorted(self.metadata['label'].dropna().unique())\n",
    "        print(f\"Unique labels found: {unique_labels}\")\n",
    "        class_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        return class_to_idx\n",
    "    \n",
    "    def _build_view_mapping(self) -> Dict[str, int]:\n",
    "        unique_views = sorted(self.metadata['view'].dropna().unique())\n",
    "        return {view: idx for idx, view in enumerate(unique_views)}\n",
    "    \n",
    "    def _save_class_mapping(self):\n",
    "        os.makedirs(os.path.dirname(self.class_to_idx_path), exist_ok=True)\n",
    "        with open(self.class_to_idx_path, 'w') as f:\n",
    "            json.dump(self.class_to_idx, f, indent=2)\n",
    "        print(f\"Saved class mapping to {self.class_to_idx_path}\")\n",
    "    \n",
    "    def _filter_valid_samples(self) -> List[int]:\n",
    "        valid_indices = []\n",
    "        for idx, row in self.metadata.iterrows():\n",
    "            image_path = os.path.join(self.images_dir, row['filename'])\n",
    "            if os.path.exists(image_path):\n",
    "                valid_indices.append(idx)\n",
    "        return valid_indices\n",
    "    \n",
    "    def _encode_label(self, label: str) -> torch.Tensor:\n",
    "        if pd.isna(label) or label not in self.class_to_idx:\n",
    "            return torch.tensor(0, dtype=torch.long)\n",
    "        return torch.tensor(self.class_to_idx[label], dtype=torch.long)\n",
    "    \n",
    "    def _encode_view(self, view: str) -> torch.Tensor:\n",
    "        if pd.isna(view):\n",
    "            return torch.zeros(len(self.view_to_idx), dtype=torch.float32)\n",
    "        view_vector = torch.zeros(len(self.view_to_idx), dtype=torch.float32)\n",
    "        if view in self.view_to_idx:\n",
    "            view_vector[self.view_to_idx[view]] = 1.0\n",
    "        return view_vector\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        row = self.metadata.iloc[actual_idx]\n",
    "        image_path = os.path.join(self.images_dir, row['filename'])\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception:\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0)) # Fallback to black image\n",
    "        \n",
    "        image_tensor = self.transform(image)\n",
    "        metadata_tensor = self._encode_view(row['view'])\n",
    "        label_tensor = self._encode_label(row['label'])\n",
    "        \n",
    "        return image_tensor, metadata_tensor, label_tensor\n",
    "    \n",
    "    def get_class_names(self) -> List[str]:\n",
    "        return [cls for cls, _ in sorted(self.class_to_idx.items(), key=lambda x: x[1])]\n",
    "    \n",
    "    def get_num_classes(self) -> int:\n",
    "        return len(self.class_to_idx)\n",
    "\n",
    "print(\"‚úÖ Modified ChestXrayDatasetSingleLabel class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca54931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ChestXrayDatasetSingleLabel with 4-class data...\n",
      "Loaded metadata with 95831 rows\n",
      "Loaded existing class mapping: {'COVID-19': 0, 'Other': 1, 'Pneumonia': 2, 'Viral': 3}\n",
      "Found 0 valid samples\n",
      "\n",
      "‚úÖ Dataset created successfully!\n",
      "üìä Dataset size: 0 samples\n",
      "üè∑Ô∏è Number of classes: 4\n",
      "üìù Class names: ['COVID-19', 'Other', 'Pneumonia', 'Viral']\n",
      "üî¢ Class mapping: {'COVID-19': 0, 'Other': 1, 'Pneumonia': 2, 'Viral': 3}\n",
      "\n",
      "üß™ Testing __getitem__ method...\n",
      "\n",
      "üéâ Single-label dataset test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# ‚úÖ Test the Modified Dataset Class\n",
    "# ==================================================\n",
    "\n",
    "# Test the single-label dataset with our 4-class data\n",
    "print(\"Testing ChestXrayDatasetSingleLabel with 4-class data...\")\n",
    "\n",
    "# Use the 4-class metadata file we created\n",
    "metadata_path = \"../data/metadata_4_classes.csv\"\n",
    "images_dir = \"../data/images_sample/\"\n",
    "class_to_idx_path = \"../data/class_to_idx_4class.json\"\n",
    "\n",
    "try:\n",
    "    # Create the single-label dataset\n",
    "    dataset_4class = ChestXrayDatasetSingleLabel(\n",
    "        metadata_path=metadata_path,\n",
    "        images_dir=images_dir,\n",
    "        class_to_idx_path=class_to_idx_path\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset created successfully!\")\n",
    "    print(f\"üìä Dataset size: {len(dataset_4class)} samples\")\n",
    "    print(f\"üè∑Ô∏è Number of classes: {dataset_4class.get_num_classes()}\")\n",
    "    print(f\"üìù Class names: {dataset_4class.get_class_names()}\")\n",
    "    print(f\"üî¢ Class mapping: {dataset_4class.class_to_idx}\")\n",
    "    \n",
    "    # Test getting a sample\n",
    "    print(f\"\\nüß™ Testing __getitem__ method...\")\n",
    "    if len(dataset_4class) > 0:\n",
    "        image_tensor, metadata_tensor, label_tensor = dataset_4class[0]\n",
    "        \n",
    "        print(f\"üì∏ Image tensor shape: {image_tensor.shape}\")\n",
    "        print(f\"üìä Metadata tensor shape: {metadata_tensor.shape}\")\n",
    "        print(f\"üè∑Ô∏è Label tensor: {label_tensor}\")\n",
    "        print(f\"üè∑Ô∏è Label tensor type: {label_tensor.dtype}\")\n",
    "        print(f\"üè∑Ô∏è Label tensor value: {label_tensor.item()}\")\n",
    "        \n",
    "        # Get the actual class name for this label\n",
    "        class_names = dataset_4class.get_class_names()\n",
    "        if label_tensor.item() < len(class_names):\n",
    "            predicted_class = class_names[label_tensor.item()]\n",
    "            print(f\"üè∑Ô∏è Predicted class: '{predicted_class}'\")\n",
    "        \n",
    "        # Show label distribution\n",
    "        print(f\"\\nüìà Label distribution in dataset:\")\n",
    "        label_counts = {}\n",
    "        for i in range(min(10, len(dataset_4class))):  # Check first 10 samples\n",
    "            _, _, label = dataset_4class[i]\n",
    "            label_val = label.item()\n",
    "            class_name = class_names[label_val] if label_val < len(class_names) else \"Unknown\"\n",
    "            label_counts[class_name] = label_counts.get(class_name, 0) + 1\n",
    "        \n",
    "        for class_name, count in label_counts.items():\n",
    "            print(f\"   {class_name}: {count} samples\")\n",
    "    \n",
    "    print(f\"\\nüéâ Single-label dataset test completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing dataset: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "265b37b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Data Augmentation pipelines defined for training!\n",
      "Loaded metadata with 95831 rows\n",
      "Loaded existing class mapping: {'Atelectasis': 0, 'Effusion': 1, 'Infiltration': 2, 'No Finding': 3}\n",
      "Found 95831 valid samples\n",
      "Loaded metadata with 95831 rows\n",
      "Loaded existing class mapping: {'Atelectasis': 0, 'Effusion': 1, 'Infiltration': 2, 'No Finding': 3}\n",
      "Found 95831 valid samples\n",
      "\n",
      "‚úÖ Datasets created with 4 classes: ['Atelectasis', 'Effusion', 'Infiltration', 'No Finding']\n",
      "üìä Dataset split: 76664 train / 19167 val\n",
      "\n",
      "‚öñÔ∏è Calculating class weights to handle imbalance...\n",
      "   Calculated weights:\n",
      "   - Atelectasis    : 0.0001\n",
      "   - Effusion       : 0.0001\n",
      "   - Infiltration   : 0.0001\n",
      "   - No Finding     : 0.0000\n",
      "\n",
      "‚úÖ DataLoaders created successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# ‚úÖ 4-Class Model Configuration with DATA AUGMENTATION\n",
    "# ==================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import pandas as pd\n",
    "from torchvision import transforms # Make sure transforms is imported\n",
    "\n",
    "# ---- NEW: Define Transformation Pipelines ----\n",
    "# 1. Training transforms with data augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(), # Randomly flip images horizontally\n",
    "    transforms.RandomRotation(10),     # Randomly rotate images by up to 10 degrees\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 2. Validation transforms (no augmentation)\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"üî• Data Augmentation pipelines defined for training!\")\n",
    "\n",
    "# ---- Paths for 4-Class Data ----\n",
    "metadata_path = \"../data/nih_metadata_prepped.csv\"\n",
    "images_dir = \"../data/image/\"\n",
    "class_to_idx_path = \"../data/class_to_idx_4class_nih.json\"\n",
    "\n",
    "# ---- Create TWO versions of the full dataset ----\n",
    "# One for training (with augmentation) and one for validation (without)\n",
    "train_full_dataset = ChestXrayDatasetSingleLabel(\n",
    "    metadata_path=metadata_path,\n",
    "    images_dir=images_dir,\n",
    "    class_to_idx_path=class_to_idx_path,\n",
    "    transform=train_transforms  # Apply training transforms\n",
    ")\n",
    "\n",
    "val_full_dataset = ChestXrayDatasetSingleLabel(\n",
    "    metadata_path=metadata_path,\n",
    "    images_dir=images_dir,\n",
    "    class_to_idx_path=class_to_idx_path,\n",
    "    transform=val_transforms   # Apply validation transforms\n",
    ")\n",
    "\n",
    "# Get class info from one of the datasets\n",
    "num_classes = train_full_dataset.get_num_classes()\n",
    "class_names = train_full_dataset.get_class_names()\n",
    "print(f\"\\n‚úÖ Datasets created with {num_classes} classes: {class_names}\")\n",
    "\n",
    "# ---- Train/Validation Split ----\n",
    "# We use a generator to ensure both splits are identical\n",
    "generator = torch.Generator().manual_seed(42) # for reproducibility\n",
    "train_size = int(0.8 * len(train_full_dataset))\n",
    "val_size = len(train_full_dataset) - train_size\n",
    "\n",
    "train_dataset, _ = random_split(train_full_dataset, [train_size, val_size], generator=generator)\n",
    "_, val_dataset = random_split(val_full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "print(f\"üìä Dataset split: {len(train_dataset)} train / {len(val_dataset)} val\")\n",
    "\n",
    "# ---- Calculate Class Weights (as before) ----\n",
    "print(\"\\n‚öñÔ∏è Calculating class weights to handle imbalance...\")\n",
    "train_labels = [train_full_dataset.metadata.iloc[i]['label'] for i in train_dataset.indices]\n",
    "class_counts = pd.Series(train_labels).value_counts()\n",
    "weights = [1.0 / class_counts[name] for name in class_names]\n",
    "class_weights_tensor = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "print(\"   Calculated weights:\")\n",
    "for name, weight in zip(class_names, class_weights_tensor):\n",
    "    print(f\"   - {name:15}: {weight:.4f}\")\n",
    "\n",
    "# ---- Create DataLoaders ----\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "print(f\"\\n‚úÖ DataLoaders created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38a51835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Initializing model components for 4-class classification...\n",
      "Detected 2 unique views in the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\cxr-multimodal\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\cxr-multimodal\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing the final blocks of the image encoder...\n",
      "‚úÖ Loss function: CrossEntropyLoss with class weights\n",
      "‚úÖ Optimizer: Adam with weight_decay=1e-5\n",
      "‚úÖ Scheduler: ReduceLROnPlateau (monitors validation loss)\n",
      "\n",
      "üéâ Model configuration completed!\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# ‚úÖ Updated Model Architecture with Advanced Optimizer\n",
    "# ==================================================\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# ---- Add necessary class definitions to make this cell self-contained ----\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, image_encoder, metadata_encoder, fusion_classifier):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.metadata_encoder = metadata_encoder\n",
    "        self.fusion_classifier = fusion_classifier\n",
    "\n",
    "    def forward(self, images, metadata):\n",
    "        img_feat = self.image_encoder(images)\n",
    "        meta_feat = self.metadata_encoder(metadata)\n",
    "        return self.fusion_classifier(img_feat, meta_feat)\n",
    "\n",
    "# ---- Main cell logic ----\n",
    "print(\"üèóÔ∏è Initializing model components for 4-class classification...\")\n",
    "\n",
    "# ---- Initialize Encoders and Classifier ----\n",
    "\n",
    "# First, get the number of views dynamically from the dataset you already created\n",
    "num_views = len(train_full_dataset.view_to_idx)\n",
    "print(f\"Detected {num_views} unique views in the dataset.\")\n",
    "\n",
    "image_encoder = ImageEncoder(pretrained=True)\n",
    "# --- NEW: UNFREEZE THE LAST FEW LAYERS OF THE MODEL ---\n",
    "print(\"Unfreezing the final blocks of the image encoder...\")\n",
    "\n",
    "# First, set the whole model to be trainable\n",
    "for param in image_encoder.backbone.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Now, freeze the first few blocks (e.g., all but the last 2 or 3)\n",
    "# EfficientNet-B0 has 8 blocks (indexed 0-7)\n",
    "for i, block in enumerate(image_encoder.backbone.features):\n",
    "    if i < 6: # This freezes blocks 0 through 5, leaving 6 and 7 trainable\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = False\n",
    "# Now, build the encoder with the correct input dimension\n",
    "metadata_encoder = MetadataEncoder(input_dim=num_views, hidden_dim=64, output_dim=128) # <--- FIXED\n",
    "fusion_classifier = FusionClassifier(\n",
    "    img_feat_dim=1280,\n",
    "    meta_feat_dim=128,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# ---- Create Multimodal Model ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultimodalModel(image_encoder, metadata_encoder, fusion_classifier).to(device)\n",
    "\n",
    "# ---- Loss Function (with weights) ----\n",
    "class_weights_tensor = class_weights_tensor.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "print(f\"‚úÖ Loss function: CrossEntropyLoss with class weights\")\n",
    "\n",
    "# ---- NEW: Optimizer with Weight Decay ----\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "print(f\"‚úÖ Optimizer: Adam with weight_decay=1e-5\")\n",
    "\n",
    "# ---- NEW: Learning Rate Scheduler (verbose argument removed) ----\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "print(f\"‚úÖ Scheduler: ReduceLROnPlateau (monitors validation loss)\")\n",
    "\n",
    "print(f\"\\nüéâ Model configuration completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64488592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated training and validation functions for single-label classification!\n",
      "üìä New metrics: Accuracy, Classification Report, Confusion Matrix\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# ‚úÖ Updated Training and Validation Functions\n",
    "# ==================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "def train_one_epoch_single_label(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"Training function for single-label classification.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (images, metadata, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        metadata = metadata.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, metadata)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Collect predictions and labels for metrics\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Print progress for first few batches\n",
    "        if batch_idx < 3:\n",
    "            print(f\"  Batch {batch_idx + 1}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return total_loss / num_batches, accuracy\n",
    "\n",
    "def validate_single_label(model, dataloader, criterion, class_names, device):\n",
    "    \"\"\"Validation function for single-label classification.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, metadata, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            metadata = metadata.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images, metadata)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(all_labels, all_preds, \n",
    "                                 target_names=class_names, \n",
    "                                 output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return total_loss / num_batches, accuracy, report, cm\n",
    "\n",
    "print(\"‚úÖ Updated training and validation functions for single-label classification!\")\n",
    "print(\"üìä New metrics: Accuracy, Classification Report, Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0f30ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Corrected validation function created!\n",
      "   - Now robust to missing classes in small validation batches.\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# ‚úÖ Updated Validation Metrics for Multi-Class Problem (Corrected)\n",
    "# ==================================================\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "def validate_single_label_enhanced(model, dataloader, criterion, class_names, device):\n",
    "    \"\"\"\n",
    "    Enhanced validation function with a fix for small validation sets where not all classes may be present.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels, all_preds, all_probabilities = [], [], []\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, metadata, labels in dataloader:\n",
    "            images, metadata, labels = images.to(device), metadata.to(device), labels.to(device)\n",
    "            outputs = model(images, metadata)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_probabilities.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probabilities = np.array(all_probabilities)\n",
    "    \n",
    "    # --- FIX: Define the full set of possible labels ---\n",
    "    # This ensures the functions work even if a class is missing from the batch.\n",
    "    possible_labels = list(range(len(class_names)))\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # --- FIX: Add the 'labels' parameter here ---\n",
    "    report = classification_report(all_labels, all_preds, \n",
    "                                 target_names=class_names, \n",
    "                                 labels=possible_labels, # Ensures report has all 4 classes\n",
    "                                 output_dict=True, zero_division=0)\n",
    "    \n",
    "    # --- FIX: Add the 'labels' parameter here ---\n",
    "    cm = confusion_matrix(all_labels, all_preds,\n",
    "                          labels=possible_labels) # Ensures matrix is always 4x4\n",
    "    \n",
    "    roc_auc = 0.0\n",
    "    per_class_auc = {name: 0.0 for name in class_names}\n",
    "    # ROC-AUC calculation requires at least one sample from each class in the batch\n",
    "    if len(np.unique(all_labels)) > 1:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(all_labels, all_probabilities, \n",
    "                                   multi_class='ovr', \n",
    "                                   average='macro',\n",
    "                                   labels=possible_labels)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è ROC-AUC calculation failed: {e}\")\n",
    "    \n",
    "    return total_loss / (num_batches or 1), accuracy, report, cm, roc_auc, per_class_auc, all_probabilities\n",
    "\n",
    "print(\"‚úÖ Corrected validation function created!\")\n",
    "print(\"   - Now robust to missing classes in small validation batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a9fe224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting 4-class training for 10 epochs...\n",
      "============================================================\n",
      "  Batch 1: Loss = 1.3866\n",
      "  Batch 2: Loss = 1.3704\n",
      "  Batch 3: Loss = 1.4263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     10\u001b[0m history \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[0;32m     13\u001b[0m }\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# --- Training Step ---\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch_single_label\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# --- Validation Step ---\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     val_loss, val_accuracy, val_report, val_cm, val_roc_auc, per_class_auc, _ \u001b[38;5;241m=\u001b[39m validate_single_label_enhanced(\n\u001b[0;32m     23\u001b[0m         model, val_loader, criterion, class_names, device\n\u001b[0;32m     24\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m, in \u001b[0;36mtrain_one_epoch_single_label\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, metadata, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m     17\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     18\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cxr-multimodal\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cxr-multimodal\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cxr-multimodal\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cxr-multimodal\\lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cxr-multimodal\\lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[7], line 95\u001b[0m, in \u001b[0;36mChestXrayDatasetSingleLabel.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     93\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_dir, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)) \u001b[38;5;66;03m# Fallback to black image\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cxr-multimodal\\lib\\site-packages\\PIL\\Image.py:986\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    984\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 986\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cxr-multimodal\\lib\\site-packages\\PIL\\ImageFile.py:370\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    368\u001b[0m         read_bytes \u001b[38;5;241m=\u001b[39m next_offset \u001b[38;5;241m-\u001b[39m offset\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 370\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct\u001b[38;5;241m.\u001b[39merror) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\cxr-multimodal\\lib\\site-packages\\PIL\\PngImagePlugin.py:1017\u001b[0m, in \u001b[0;36mPngImageFile.load_read\u001b[1;34m(self, read_bytes)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     read_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(read_bytes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__idat)\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__idat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__idat \u001b[38;5;241m-\u001b[39m read_bytes\n\u001b[1;32m-> 1017\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# ‚úÖ Final Training Loop with History Tracking\n",
    "# ==================================================\n",
    "\n",
    "num_epochs = 10 \n",
    "print(f\"üöÄ Starting 4-class training for {num_epochs} epochs...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# NEW: Create a history object to store metrics from each epoch\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_acc': [], 'val_acc': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Step ---\n",
    "    train_loss, train_accuracy = train_one_epoch_single_label(\n",
    "        model, train_loader, optimizer, criterion, device\n",
    "    )\n",
    "    \n",
    "    # --- Validation Step ---\n",
    "    val_loss, val_accuracy, val_report, val_cm, val_roc_auc, per_class_auc, _ = validate_single_label_enhanced(\n",
    "        model, val_loader, criterion, class_names, device\n",
    "    )\n",
    "    \n",
    "    # --- Update the Learning Rate Scheduler ---\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # --- NEW: Record metrics in our history object ---\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_accuracy)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_accuracy)\n",
    "    \n",
    "    # --- Logging for the Epoch (Your existing code, preserved) ---\n",
    "    print(f\"\\nüìÖ Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"   Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2%}\")\n",
    "    print(f\"   Val Loss:   {val_loss:.4f} | Val Accuracy:   {val_accuracy:.2%}\")\n",
    "    print(f\"   Val ROC-AUC (macro): {val_roc_auc:.4f}\")\n",
    "    \n",
    "    # --- Per-Class Metrics Table (Your existing code, preserved) ---\n",
    "    print(f\"\\nüìà Per-class Validation Metrics:\")\n",
    "    print(f\"   {'Class':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
    "    print(f\"   {'-'*15} {'-'*10} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "    for class_name in class_names:\n",
    "        if class_name in val_report:\n",
    "            precision = val_report[class_name]['precision']\n",
    "            recall = val_report[class_name]['recall']\n",
    "            f1 = val_report[class_name]['f1-score']\n",
    "            support = val_report[class_name]['support']\n",
    "            print(f\"   {class_name:<15} {precision:<10.3f} {recall:<10.3f} {f1:<10.3f} {support:<10}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# ‚úÖ Step 1: Explainability Imports & Setup\n",
    "# ==================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2  # OpenCV for heatmap visualization\n",
    "from captum.attr import LayerGradCam\n",
    "import shap\n",
    "\n",
    "# Helper function to denormalize images for visualization\n",
    "def denormalize_image(tensor):\n",
    "    \"\"\"Reverses the ImageNet normalization for a PyTorch tensor.\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    # Move tensor to CPU for numpy conversion\n",
    "    tensor = tensor.clone().cpu()\n",
    "    \n",
    "    # Denormalize\n",
    "    tensor.mul_(std).add_(mean)\n",
    "    \n",
    "    # Convert to numpy array suitable for plotting\n",
    "    tensor = tensor.numpy().transpose(1, 2, 0)\n",
    "    \n",
    "    # Clip values to be between 0 and 1\n",
    "    tensor = np.clip(tensor, 0, 1)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "print(\"‚úÖ Explainability libraries imported successfully.\")\n",
    "print(\"   - Captum for Grad-CAM\")\n",
    "print(\"   - SHAP for feature attribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12340ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# ‚úÖ Step 2: Grad-CAM Explainability (Corrected for OpenCV)\n",
    "# ==================================================\n",
    "import torch.nn as nn\n",
    "\n",
    "# A wrapper is needed for Captum\n",
    "class ModelWrapperForGradCam(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, image, metadata):\n",
    "        return self.model(image, metadata)\n",
    "\n",
    "# --- FIX IS HERE: This function now handles the data type conversion ---\n",
    "def overlay_heatmap(heatmap, image, alpha=0.5, colormap=cv2.COLORMAP_JET):\n",
    "    \"\"\"Overlays a heatmap onto a numpy image, ensuring data types match.\"\"\"\n",
    "    # Convert the base image to uint8 [0, 255] format if it's float [0, 1]\n",
    "    if image.dtype == np.float32 or image.dtype == np.float64:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        \n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "    \n",
    "    # Both image and heatmap are now uint8, so they can be blended\n",
    "    overlayed_image = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n",
    "    return overlayed_image\n",
    "\n",
    "# --- Setup Grad-CAM ---\n",
    "model.eval()\n",
    "wrapped_model = ModelWrapperForGradCam(model)\n",
    "target_layer = model.image_encoder.backbone.features[-1]\n",
    "grad_cam = LayerGradCam(wrapped_model, target_layer)\n",
    "\n",
    "print(\"‚úÖ Grad-CAM setup complete.\")\n",
    "\n",
    "# --- Run Analysis on a Sample ---\n",
    "image_tensor, metadata_tensor, label_tensor = next(iter(val_loader))\n",
    "image_tensor, metadata_tensor, label_tensor = image_tensor[0:1], metadata_tensor[0:1], label_tensor[0:1]\n",
    "image_tensor, metadata_tensor = image_tensor.to(device), metadata_tensor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(image_tensor, metadata_tensor)\n",
    "    prediction = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "target_class = prediction\n",
    "true_label = label_tensor.item()\n",
    "\n",
    "print(f\"Analyzing sample with True Label: '{class_names[true_label]}', Predicted: '{class_names[prediction]}'\")\n",
    "\n",
    "# Full-Model Grad-CAM\n",
    "attribution_full = grad_cam.attribute(inputs=image_tensor, additional_forward_args=metadata_tensor, target=target_class)\n",
    "heatmap_full = attribution_full.squeeze().cpu().detach().numpy()\n",
    "heatmap_full = np.maximum(heatmap_full, 0) / (np.max(heatmap_full) + 1e-8)\n",
    "\n",
    "# Image-Only Grad-CAM\n",
    "dummy_metadata = torch.zeros_like(metadata_tensor).to(device)\n",
    "attribution_img_only = grad_cam.attribute(inputs=image_tensor, additional_forward_args=dummy_metadata, target=target_class)\n",
    "heatmap_img_only = attribution_img_only.squeeze().cpu().detach().numpy()\n",
    "heatmap_img_only = np.maximum(heatmap_img_only, 0) / (np.max(heatmap_img_only) + 1e-8)\n",
    "\n",
    "# --- Visualize the Comparison ---\n",
    "raw_image_for_viz = denormalize_image(image_tensor.squeeze())\n",
    "overlay_full = overlay_heatmap(heatmap_full, raw_image_for_viz)\n",
    "overlay_img_only = overlay_heatmap(heatmap_img_only, raw_image_for_viz)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "axs[0].imshow(raw_image_for_viz)\n",
    "axs[0].set_title(f\"Original Image\\nTrue: {class_names[true_label]}\")\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(overlay_img_only)\n",
    "axs[1].set_title(\"Image-Only Grad-CAM Focus\")\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(overlay_full)\n",
    "axs[2].set_title(\"Full-Model Grad-CAM Focus\\n(with Metadata)\")\n",
    "axs[2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# ‚úÖ Step 5: Visualize All Performance Metrics\n",
    "# ==================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# The 'val_report' and 'val_cm' variables are from the last epoch of the training loop\n",
    "final_val_report = val_report\n",
    "final_val_cm = val_cm\n",
    "\n",
    "# --- 1. Plot Loss and Accuracy Curves ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot Loss\n",
    "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Validation Loss', marker='o')\n",
    "ax1.set_title('Loss Curves', fontsize=16)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot Accuracy\n",
    "ax2.plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
    "ax2.plot(history['val_acc'], label='Validation Accuracy', marker='o')\n",
    "ax2.set_title('Accuracy Curves', fontsize=16)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 2. Plot Confusion Matrix ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(final_val_cm, annot=True, fmt='g', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 14})\n",
    "plt.xlabel('Predicted Labels', fontsize=12)\n",
    "plt.ylabel('True Labels', fontsize=12)\n",
    "plt.title('Final Confusion Matrix', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 3. Plot Performance Metrics (Precision, Recall, F1-Score) ---\n",
    "# Extract metrics from the classification report into a pandas DataFrame\n",
    "report_df = pd.DataFrame(final_val_report).iloc[:-1, :].T\n",
    "# Ensure we only plot the classes that are in our class_names list\n",
    "report_df = report_df[report_df.index.isin(class_names)] \n",
    "\n",
    "report_df[['precision', 'recall', 'f1-score']].plot(kind='bar', figsize=(12, 7), width=0.8)\n",
    "plt.title('Performance Metrics per Class', fontsize=16)\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595afef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# ‚úÖ Save the Final NIH Model\n",
    "# ==================================================\n",
    "\n",
    "# Define a new path for the trained NIH model\n",
    "model_save_path = \"../models/nih_4class_model.pth\"\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"üéâ NIH model saved successfully to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cxr-multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
