{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b9d2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "  Total samples: 116\n",
      "  Number of classes: 28\n",
      "  Number of views: 7\n",
      "  Classes: ['Aspergillosis', 'Aspiration', 'Bacterial', 'COVID-19', 'Chlamydophila', 'E.Coli', 'Fungal', 'H1N1', 'Herpes', 'Influenza', 'Klebsiella', 'Legionella', 'Lipoid', 'MERS-CoV', 'MRSA', 'Mycoplasma', 'No Finding', 'Nocardia', 'Pneumocystis', 'Pneumonia', 'SARS', 'Staphylococcus', 'Streptococcus', 'Tuberculosis', 'Unknown', 'Varicella', 'Viral', 'todo']\n",
      "  Views: ['AP', 'AP Erect', 'AP Supine', 'Axial', 'Coronal', 'L', 'PA']\n",
      "\n",
      "Batch 1:\n",
      "  Image tensor shape: torch.Size([4, 3, 224, 224])\n",
      "  Metadata tensor shape: torch.Size([4, 7])\n",
      "  Label tensor shape: torch.Size([4, 28])\n",
      "  Image dtype: torch.float32\n",
      "  Metadata dtype: torch.float32\n",
      "  Label dtype: torch.float32\n",
      "  Sample label vectors:\n",
      "    Sample 0: ['COVID-19', 'Pneumonia', 'Viral']\n",
      "    Sample 1: ['COVID-19', 'Pneumonia', 'Viral']\n",
      "\n",
      "Dataset test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, Dict, List\n",
    "import glob\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Chest X-ray images with multi-hot encoded labels.\n",
    "    \n",
    "    Reads metadata from CSV file, loads images, and returns:\n",
    "    - image_tensor: Preprocessed image (224x224, normalized)\n",
    "    - metadata_tensor: One-hot encoded view information\n",
    "    - label_tensor: Multi-hot encoded finding labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_path: str, images_dir: str, class_to_idx_path: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            metadata_path: Path to CSV file with metadata\n",
    "            images_dir: Directory containing images\n",
    "            class_to_idx_path: Path to save/load class_to_idx mapping\n",
    "        \"\"\"\n",
    "        self.metadata_path = metadata_path\n",
    "        self.images_dir = images_dir\n",
    "        self.class_to_idx_path = class_to_idx_path or \"../data/class_to_idx.json\"\n",
    "        \n",
    "        # Load metadata\n",
    "        self.metadata = pd.read_csv(metadata_path)\n",
    "        \n",
    "        # Build class_to_idx mapping if not exists\n",
    "        if os.path.exists(self.class_to_idx_path):\n",
    "            with open(self.class_to_idx_path, 'r') as f:\n",
    "                self.class_to_idx = json.load(f)\n",
    "        else:\n",
    "            self.class_to_idx = self._build_class_mapping()\n",
    "            self._save_class_mapping()\n",
    "        \n",
    "        # Build view mapping\n",
    "        self.view_to_idx = self._build_view_mapping()\n",
    "        \n",
    "        # Image preprocessing transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "        ])\n",
    "        \n",
    "        # Filter out samples where image doesn't exist\n",
    "        self.valid_indices = self._filter_valid_samples()\n",
    "        \n",
    "    def _build_class_mapping(self) -> Dict[str, int]:\n",
    "        \"\"\"Build class_to_idx mapping from finding column.\"\"\"\n",
    "        all_classes = set()\n",
    "        \n",
    "        for finding in self.metadata['finding'].dropna():\n",
    "            # Split on '/' for multiple diseases\n",
    "            classes = [cls.strip() for cls in finding.split('/')]\n",
    "            all_classes.update(classes)\n",
    "        \n",
    "        # Sort for consistent ordering\n",
    "        sorted_classes = sorted(list(all_classes))\n",
    "        return {cls: idx for idx, cls in enumerate(sorted_classes)}\n",
    "    \n",
    "    def _build_view_mapping(self) -> Dict[str, int]:\n",
    "        \"\"\"Build view_to_idx mapping from view column.\"\"\"\n",
    "        unique_views = sorted(self.metadata['view'].dropna().unique())\n",
    "        return {view: idx for idx, view in enumerate(unique_views)}\n",
    "    \n",
    "    def _save_class_mapping(self):\n",
    "        \"\"\"Save class_to_idx mapping to JSON file.\"\"\"\n",
    "        os.makedirs(os.path.dirname(self.class_to_idx_path), exist_ok=True)\n",
    "        with open(self.class_to_idx_path, 'w') as f:\n",
    "            json.dump(self.class_to_idx, f, indent=2)\n",
    "    \n",
    "    def _filter_valid_samples(self) -> List[int]:\n",
    "        \"\"\"Filter out samples where image file doesn't exist.\"\"\"\n",
    "        valid_indices = []\n",
    "        for idx, row in self.metadata.iterrows():\n",
    "            image_path = os.path.join(self.images_dir, row['filename'])\n",
    "            if os.path.exists(image_path):\n",
    "                valid_indices.append(idx)\n",
    "        return valid_indices\n",
    "    \n",
    "    def _encode_finding(self, finding: str) -> torch.Tensor:\n",
    "        \"\"\"Convert finding string to multi-hot encoded vector.\"\"\"\n",
    "        if pd.isna(finding):\n",
    "            return torch.zeros(len(self.class_to_idx), dtype=torch.float32)\n",
    "        \n",
    "        # Split on '/' for multiple diseases\n",
    "        classes = [cls.strip() for cls in finding.split('/')]\n",
    "        \n",
    "        # Create multi-hot vector\n",
    "        label_vector = torch.zeros(len(self.class_to_idx), dtype=torch.float32)\n",
    "        for cls in classes:\n",
    "            if cls in self.class_to_idx:\n",
    "                label_vector[self.class_to_idx[cls]] = 1.0\n",
    "        \n",
    "        return label_vector\n",
    "    \n",
    "    def _encode_view(self, view: str) -> torch.Tensor:\n",
    "        \"\"\"Convert view string to one-hot encoded vector.\"\"\"\n",
    "        if pd.isna(view):\n",
    "            return torch.zeros(len(self.view_to_idx), dtype=torch.float32)\n",
    "        \n",
    "        view_vector = torch.zeros(len(self.view_to_idx), dtype=torch.float32)\n",
    "        if view in self.view_to_idx:\n",
    "            view_vector[self.view_to_idx[view]] = 1.0\n",
    "        \n",
    "        return view_vector\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            image_tensor: Preprocessed image (3, 224, 224)\n",
    "            metadata_tensor: One-hot encoded view (num_views,)\n",
    "            label_tensor: Multi-hot encoded finding (num_classes,)\n",
    "        \"\"\"\n",
    "        # Get the actual index in metadata\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        row = self.metadata.iloc[actual_idx]\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image_path = os.path.join(self.images_dir, row['filename'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = self.transform(image)\n",
    "        \n",
    "        # Encode metadata and labels\n",
    "        metadata_tensor = self._encode_view(row['view'])\n",
    "        label_tensor = self._encode_finding(row['finding'])\n",
    "        \n",
    "        return image_tensor, metadata_tensor, label_tensor\n",
    "    \n",
    "    def get_class_names(self) -> List[str]:\n",
    "        \"\"\"Get list of class names in order.\"\"\"\n",
    "        return [cls for cls, _ in sorted(self.class_to_idx.items(), key=lambda x: x[1])]\n",
    "    \n",
    "    def get_view_names(self) -> List[str]:\n",
    "        \"\"\"Get list of view names in order.\"\"\"\n",
    "        return [view for view, _ in sorted(self.view_to_idx.items(), key=lambda x: x[1])]\n",
    "    \n",
    "    def print_dataset_info(self):\n",
    "        \"\"\"Print dataset information.\"\"\"\n",
    "        print(f\"Dataset Info:\")\n",
    "        print(f\"  Total samples: {len(self)}\")\n",
    "        print(f\"  Number of classes: {len(self.class_to_idx)}\")\n",
    "        print(f\"  Number of views: {len(self.view_to_idx)}\")\n",
    "        print(f\"  Classes: {self.get_class_names()}\")\n",
    "        print(f\"  Views: {self.get_view_names()}\")\n",
    "\n",
    "# Test the dataset\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize dataset\n",
    "    dataset = ChestXrayDataset(\n",
    "        metadata_path=\"../data/metadata_sample_minimal.csv\",\n",
    "        images_dir=\"../data/images_sample/\"\n",
    "    )\n",
    "    \n",
    "    # Print dataset info\n",
    "    dataset.print_dataset_info()\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    # Test a batch\n",
    "    for batch_idx, (images, metadata, labels) in enumerate(dataloader):\n",
    "        print(f\"\\nBatch {batch_idx + 1}:\")\n",
    "        print(f\"  Image tensor shape: {images.shape}\")\n",
    "        print(f\"  Metadata tensor shape: {metadata.shape}\")\n",
    "        print(f\"  Label tensor shape: {labels.shape}\")\n",
    "        \n",
    "        # Show some details\n",
    "        print(f\"  Image dtype: {images.dtype}\")\n",
    "        print(f\"  Metadata dtype: {metadata.dtype}\")\n",
    "        print(f\"  Label dtype: {labels.dtype}\")\n",
    "        \n",
    "        # Show sample labels\n",
    "        print(f\"  Sample label vectors:\")\n",
    "        for i in range(min(2, labels.shape[0])):\n",
    "            active_classes = torch.nonzero(labels[i]).squeeze()\n",
    "            if active_classes.numel() > 0:\n",
    "                class_names = [dataset.get_class_names()[idx.item()] for idx in active_classes]\n",
    "                print(f\"    Sample {i}: {class_names}\")\n",
    "            else:\n",
    "                print(f\"    Sample {i}: No active classes\")\n",
    "        \n",
    "        # Only show first batch for testing\n",
    "        break\n",
    "    \n",
    "    print(\"\\nDataset test completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c79af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\cxr-multimodal\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\cxr-multimodal\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageEncoder initialized successfully!\n",
      "Feature dimension: 1280\n",
      "\n",
      "Input shape: torch.Size([4, 3, 224, 224])\n",
      "Output shape: torch.Size([4, 1280])\n",
      "Expected shape: [4, 1280]\n",
      "✅ Output shape is correct!\n",
      "\n",
      "Output statistics:\n",
      "  Mean: 0.1566\n",
      "  Std: 0.2077\n",
      "  Min: -0.1969\n",
      "  Max: 1.0907\n",
      "\n",
      "Parameter status:\n",
      "  Frozen parameters: 211/211\n",
      "  Trainable parameters: 0/211\n",
      "\n",
      "ImageEncoder test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Image encoder using EfficientNet-B0 pretrained model.\n",
    "    \n",
    "    Features:\n",
    "    - Loads pretrained EfficientNet-B0\n",
    "    - Freezes all backbone parameters\n",
    "    - Replaces classifier with Identity to output 1280-dim features\n",
    "    - Input: (B, 3, 224, 224), Output: (B, 1280)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the ImageEncoder.\n",
    "        \n",
    "        Args:\n",
    "            pretrained: Whether to load pretrained weights\n",
    "        \"\"\"\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        \n",
    "        # Load pretrained EfficientNet-B0\n",
    "        self.backbone = models.efficientnet_b0(pretrained=pretrained)\n",
    "        \n",
    "        # Freeze all parameters in the backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Replace the classifier with Identity to output raw features\n",
    "        # EfficientNet-B0 classifier outputs 1000 classes, we want 1280 features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        # Get the feature dimension (1280 for EfficientNet-B0)\n",
    "        self.feature_dim = self.backbone.classifier.in_features if hasattr(self.backbone.classifier, 'in_features') else 1280\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (B, 3, 224, 224)\n",
    "            \n",
    "        Returns:\n",
    "            Feature tensor of shape (B, 1280)\n",
    "        \"\"\"\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def get_feature_dim(self) -> int:\n",
    "        \"\"\"Get the output feature dimension.\"\"\"\n",
    "        return self.feature_dim\n",
    "\n",
    "# Test the ImageEncoder\n",
    "if __name__ == \"__main__\":\n",
    "    # Create encoder\n",
    "    encoder = ImageEncoder(pretrained=True)\n",
    "    \n",
    "    print(\"ImageEncoder initialized successfully!\")\n",
    "    print(f\"Feature dimension: {encoder.get_feature_dim()}\")\n",
    "    \n",
    "    # Test with random batch\n",
    "    batch_size = 4\n",
    "    random_batch = torch.randn(batch_size, 3, 224, 224)\n",
    "    \n",
    "    print(f\"\\nInput shape: {random_batch.shape}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = encoder(random_batch)\n",
    "    \n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Expected shape: [{batch_size}, 1280]\")\n",
    "    \n",
    "    # Verify the output shape\n",
    "    expected_shape = (batch_size, 1280)\n",
    "    if output.shape == expected_shape:\n",
    "        print(\"✅ Output shape is correct!\")\n",
    "    else:\n",
    "        print(\"❌ Output shape is incorrect!\")\n",
    "    \n",
    "    # Show some statistics\n",
    "    print(f\"\\nOutput statistics:\")\n",
    "    print(f\"  Mean: {output.mean().item():.4f}\")\n",
    "    print(f\"  Std: {output.std().item():.4f}\")\n",
    "    print(f\"  Min: {output.min().item():.4f}\")\n",
    "    print(f\"  Max: {output.max().item():.4f}\")\n",
    "    \n",
    "    # Check if parameters are frozen\n",
    "    frozen_params = sum(1 for param in encoder.backbone.parameters() if not param.requires_grad)\n",
    "    total_params = sum(1 for param in encoder.backbone.parameters())\n",
    "    print(f\"\\nParameter status:\")\n",
    "    print(f\"  Frozen parameters: {frozen_params}/{total_params}\")\n",
    "    print(f\"  Trainable parameters: {total_params - frozen_params}/{total_params}\")\n",
    "    \n",
    "    print(\"\\nImageEncoder test completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7b6aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetadataEncoder initialized successfully!\n",
      "Input dimension: 7\n",
      "Hidden dimension: 64\n",
      "Output dimension: 128\n",
      "\n",
      "Input shape: torch.Size([4, 7])\n",
      "Output shape: torch.Size([4, 128])\n",
      "Expected shape: [4, 128]\n",
      "✅ Output shape is correct!\n",
      "\n",
      "Output statistics:\n",
      "  Mean: 0.0814\n",
      "  Std: 0.1235\n",
      "  Min: 0.0000\n",
      "  Max: 0.6653\n",
      "\n",
      "Parameter count:\n",
      "  Total parameters: 8,832\n",
      "  Trainable parameters: 8,832\n",
      "\n",
      "Network architecture:\n",
      "  Layer 0: Linear(7 → 64)\n",
      "  Layer 1: ReLU()\n",
      "  Layer 2: Linear(64 → 128)\n",
      "  Layer 3: ReLU()\n",
      "\n",
      "MetadataEncoder test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MetadataEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Metadata encoder using a simple feedforward network.\n",
    "    \n",
    "    Features:\n",
    "    - Input: (B, 7) float tensor (normalized metadata)\n",
    "    - Two-layer feedforward network with ReLU activations\n",
    "    - Output: (B, 128) metadata embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 7, hidden_dim: int = 64, output_dim: int = 128):\n",
    "        \"\"\"\n",
    "        Initialize the MetadataEncoder.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input feature dimension (default: 7)\n",
    "            hidden_dim: Hidden layer dimension (default: 64)\n",
    "            output_dim: Output embedding dimension (default: 128)\n",
    "        \"\"\"\n",
    "        super(MetadataEncoder, self).__init__()\n",
    "        \n",
    "        # Define the feedforward network\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Store dimensions for reference\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (B, 7)\n",
    "            \n",
    "        Returns:\n",
    "            Metadata embedding tensor of shape (B, 128)\n",
    "        \"\"\"\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def get_input_dim(self) -> int:\n",
    "        \"\"\"Get the input dimension.\"\"\"\n",
    "        return self.input_dim\n",
    "    \n",
    "    def get_output_dim(self) -> int:\n",
    "        \"\"\"Get the output dimension.\"\"\"\n",
    "        return self.output_dim\n",
    "    \n",
    "    def get_hidden_dim(self) -> int:\n",
    "        \"\"\"Get the hidden dimension.\"\"\"\n",
    "        return self.hidden_dim\n",
    "\n",
    "# Test the MetadataEncoder\n",
    "if __name__ == \"__main__\":\n",
    "    # Create encoder\n",
    "    encoder = MetadataEncoder()\n",
    "    \n",
    "    print(\"MetadataEncoder initialized successfully!\")\n",
    "    print(f\"Input dimension: {encoder.get_input_dim()}\")\n",
    "    print(f\"Hidden dimension: {encoder.get_hidden_dim()}\")\n",
    "    print(f\"Output dimension: {encoder.get_output_dim()}\")\n",
    "    \n",
    "    # Test with random batch\n",
    "    batch_size = 4\n",
    "    random_batch = torch.randn(batch_size, 7)\n",
    "    \n",
    "    print(f\"\\nInput shape: {random_batch.shape}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = encoder(random_batch)\n",
    "    \n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Expected shape: [{batch_size}, 128]\")\n",
    "    \n",
    "    # Verify the output shape\n",
    "    expected_shape = (batch_size, 128)\n",
    "    if output.shape == expected_shape:\n",
    "        print(\"✅ Output shape is correct!\")\n",
    "    else:\n",
    "        print(\"❌ Output shape is incorrect!\")\n",
    "    \n",
    "    # Show some statistics\n",
    "    print(f\"\\nOutput statistics:\")\n",
    "    print(f\"  Mean: {output.mean().item():.4f}\")\n",
    "    print(f\"  Std: {output.std().item():.4f}\")\n",
    "    print(f\"  Min: {output.min().item():.4f}\")\n",
    "    print(f\"  Max: {output.max().item():.4f}\")\n",
    "    \n",
    "    # Check parameter count\n",
    "    total_params = sum(p.numel() for p in encoder.parameters())\n",
    "    trainable_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "    print(f\"\\nParameter count:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Show layer details\n",
    "    print(f\"\\nNetwork architecture:\")\n",
    "    for i, layer in enumerate(encoder.encoder):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            print(f\"  Layer {i}: Linear({layer.in_features} → {layer.out_features})\")\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            print(f\"  Layer {i}: ReLU()\")\n",
    "    \n",
    "    print(\"\\nMetadataEncoder test completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "379a26e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FusionClassifier initialized successfully!\n",
      "Image feature dimension: 1280\n",
      "Metadata feature dimension: 128\n",
      "Attention dimension: 256\n",
      "Fusion dimension: 1664\n",
      "Number of classes: 28\n",
      "\n",
      "Input shapes:\n",
      "  Image features: torch.Size([4, 1280])\n",
      "  Metadata features: torch.Size([4, 128])\n",
      "\n",
      "Output shape: torch.Size([4, 28])\n",
      "Expected shape: [4, 28]\n",
      "✅ Output shape is correct!\n",
      "\n",
      "Output statistics:\n",
      "  Mean: 0.0064\n",
      "  Std: 0.1564\n",
      "  Min: -0.4314\n",
      "  Max: 0.3584\n",
      "\n",
      "Parameter count:\n",
      "  Total parameters: 1,392,924\n",
      "  Trainable parameters: 1,392,924\n",
      "\n",
      "Network architecture:\n",
      "  Gating mechanism:\n",
      "    Gate: Linear(128 → 1280)\n",
      "  Attention mechanism:\n",
      "    Image projection: Linear(1280 → 256)\n",
      "    Metadata projection: Linear(128 → 256)\n",
      "  Classifier:\n",
      "    Layer 0: Linear(1664 → 512)\n",
      "    Layer 1: ReLU()\n",
      "    Layer 2: Dropout(0.3)\n",
      "    Layer 3: Linear(512 → 28)\n",
      "\n",
      "FusionClassifier test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FusionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Fusion classifier that combines image and metadata features using:\n",
    "    - Concatenation\n",
    "    - Gating mechanism\n",
    "    - Attention mechanism\n",
    "    - Feedforward classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_feat_dim: int = 1280, meta_feat_dim: int = 128, \n",
    "                 num_classes: int = 28, hidden_dim: int = 512, \n",
    "                 attention_dim: int = 256, dropout: float = 0.3):\n",
    "        \"\"\"\n",
    "        Initialize the FusionClassifier.\n",
    "        \n",
    "        Args:\n",
    "            img_feat_dim: Image feature dimension (default: 1280)\n",
    "            meta_feat_dim: Metadata feature dimension (default: 128)\n",
    "            num_classes: Number of output classes (default: 28)\n",
    "            hidden_dim: Hidden layer dimension (default: 512)\n",
    "            attention_dim: Attention projection dimension (default: 256)\n",
    "            dropout: Dropout rate (default: 0.3)\n",
    "        \"\"\"\n",
    "        super(FusionClassifier, self).__init__()\n",
    "        \n",
    "        self.img_feat_dim = img_feat_dim\n",
    "        self.meta_feat_dim = meta_feat_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        # Gating mechanism\n",
    "        self.gate = nn.Linear(meta_feat_dim, img_feat_dim)\n",
    "        \n",
    "        # Attention mechanism - project to common space\n",
    "        self.img_proj = nn.Linear(img_feat_dim, attention_dim)\n",
    "        self.meta_proj = nn.Linear(meta_feat_dim, attention_dim)\n",
    "        \n",
    "        # Fusion dimension after concatenation\n",
    "        fusion_dim = img_feat_dim + meta_feat_dim + attention_dim  # 1280 + 128 + 256 = 1664\n",
    "        \n",
    "        # Feedforward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, img_feat: torch.Tensor, meta_feat: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the fusion classifier.\n",
    "        \n",
    "        Args:\n",
    "            img_feat: Image features of shape (B, 1280)\n",
    "            meta_feat: Metadata features of shape (B, 128)\n",
    "            \n",
    "        Returns:\n",
    "            Logits of shape (B, 28)\n",
    "        \"\"\"\n",
    "        batch_size = img_feat.size(0)\n",
    "        \n",
    "        # 1. Gating mechanism\n",
    "        gate = torch.sigmoid(self.gate(meta_feat))  # (B, 1280)\n",
    "        gated_img = img_feat * gate  # (B, 1280)\n",
    "        \n",
    "        # 2. Attention mechanism\n",
    "        # Project both features to common space\n",
    "        img_proj = self.img_proj(gated_img)  # (B, 256)\n",
    "        meta_proj = self.meta_proj(meta_feat)  # (B, 256)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = F.softmax(torch.sum(img_proj * meta_proj, dim=1, keepdim=True), dim=0)  # (B, 1)\n",
    "        \n",
    "        # Apply attention to image features\n",
    "        attended_img = img_proj * attention_weights  # (B, 256)\n",
    "        \n",
    "        # 3. Concatenation fusion\n",
    "        fusion_vector = torch.cat([gated_img, meta_feat, attended_img], dim=1)  # (B, 1280 + 128 + 256)\n",
    "        \n",
    "        # 4. Feedforward classifier\n",
    "        logits = self.classifier(fusion_vector)  # (B, 28)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def get_fusion_dim(self) -> int:\n",
    "        \"\"\"Get the fusion vector dimension.\"\"\"\n",
    "        return self.img_feat_dim + self.meta_feat_dim + self.attention_dim\n",
    "    \n",
    "    def get_num_classes(self) -> int:\n",
    "        \"\"\"Get the number of output classes.\"\"\"\n",
    "        return self.num_classes\n",
    "\n",
    "# Test the FusionClassifier\n",
    "if __name__ == \"__main__\":\n",
    "    # Create fusion classifier\n",
    "    fusion_model = FusionClassifier()\n",
    "    \n",
    "    print(\"FusionClassifier initialized successfully!\")\n",
    "    print(f\"Image feature dimension: {fusion_model.img_feat_dim}\")\n",
    "    print(f\"Metadata feature dimension: {fusion_model.meta_feat_dim}\")\n",
    "    print(f\"Attention dimension: {fusion_model.attention_dim}\")\n",
    "    print(f\"Fusion dimension: {fusion_model.get_fusion_dim()}\")\n",
    "    print(f\"Number of classes: {fusion_model.get_num_classes()}\")\n",
    "    \n",
    "    # Test with dummy data\n",
    "    batch_size = 4\n",
    "    img_feat = torch.randn(batch_size, 1280)\n",
    "    meta_feat = torch.randn(batch_size, 128)\n",
    "    \n",
    "    print(f\"\\nInput shapes:\")\n",
    "    print(f\"  Image features: {img_feat.shape}\")\n",
    "    print(f\"  Metadata features: {meta_feat.shape}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = fusion_model(img_feat, meta_feat)\n",
    "    \n",
    "    print(f\"\\nOutput shape: {output.shape}\")\n",
    "    print(f\"Expected shape: [{batch_size}, 28]\")\n",
    "    \n",
    "    # Verify the output shape\n",
    "    expected_shape = (batch_size, 28)\n",
    "    if output.shape == expected_shape:\n",
    "        print(\"✅ Output shape is correct!\")\n",
    "    else:\n",
    "        print(\"❌ Output shape is incorrect!\")\n",
    "    \n",
    "    # Show some statistics\n",
    "    print(f\"\\nOutput statistics:\")\n",
    "    print(f\"  Mean: {output.mean().item():.4f}\")\n",
    "    print(f\"  Std: {output.std().item():.4f}\")\n",
    "    print(f\"  Min: {output.min().item():.4f}\")\n",
    "    print(f\"  Max: {output.max().item():.4f}\")\n",
    "    \n",
    "    # Check parameter count\n",
    "    total_params = sum(p.numel() for p in fusion_model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in fusion_model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nParameter count:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Show layer details\n",
    "    print(f\"\\nNetwork architecture:\")\n",
    "    print(\"  Gating mechanism:\")\n",
    "    print(f\"    Gate: Linear({fusion_model.meta_feat_dim} → {fusion_model.img_feat_dim})\")\n",
    "    print(\"  Attention mechanism:\")\n",
    "    print(f\"    Image projection: Linear({fusion_model.img_feat_dim} → {fusion_model.attention_dim})\")\n",
    "    print(f\"    Metadata projection: Linear({fusion_model.meta_feat_dim} → {fusion_model.attention_dim})\")\n",
    "    print(\"  Classifier:\")\n",
    "    for i, layer in enumerate(fusion_model.classifier):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            print(f\"    Layer {i}: Linear({layer.in_features} → {layer.out_features})\")\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            print(f\"    Layer {i}: ReLU()\")\n",
    "        elif isinstance(layer, nn.Dropout):\n",
    "            print(f\"    Layer {i}: Dropout({layer.p})\")\n",
    "    \n",
    "    print(\"\\nFusionClassifier test completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cxr-multimodal)",
   "language": "python",
   "name": "cxr-multimodal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
